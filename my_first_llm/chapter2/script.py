import tiktoken
import torch
from torch.utils.data import Dataset, DataLoader

# 1. Tokeniza√ß√£o com BPE (Byte Pair Encoding)
tokenizer = tiktoken.get_encoding("gpt2")

# PyTorch Class
class GPTDatasetV1(Dataset):
  def __init__(self, txt, tokenizer, max_length, stride):
    self.input_ids = []
    self.target_ids = []

    token_ids = tokenizer.encode(txt) # 1

    # 2. Cria√ß√£o de pares (entrada-alvo) para treino
    for i in range(0, len(token_ids) - max_length, stride):
      input_chunk = token_ids[i:i + max_length]
      target_chunk = token_ids[i + 1: i + max_length + 1]
      self.input_ids.append(torch.tensor(input_chunk))
      self.target_ids.append(torch.tensor(target_chunk))

  def __len__(self):
    return len(self.input_ids)
  
  def __getitem__(self, idx):
    return self.input_ids[idx], self.target_ids[idx]
  

# Data Loader that use GPTDatasetV1
def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):
  tokenizer = tiktoken.get_encoding("gpt2")
  dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)
  dataloader = DataLoader(
    dataset,
    batch_size=batch_size,
    shuffle=shuffle,
    drop_last=drop_last,
    num_workers=num_workers
  )
  return dataloader


with open("the-verdict.txt", "r", encoding="utf-8") as f:
  raw_text = f.read()

# 3. Embedding dos tokens
# Cada token √© mapeado para um vetor cont√≠nuo de dimens√£o 256.
# Se inputs.shape = (8, 4), ou seja, 8 sequ√™ncias (pq 8 sequencias?) com 4 tokens cada:
# token_embeddings.shape = (8, 4, 256)
# Isso √© chamado de token embeddings ‚Äî eles capturam propriedades sem√¢nticas dos tokens.
vocab_size = 50257
output_dim = 256
token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)

max_length = 4

'''
dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)
data_iter = iter(dataloader)


first_batch = next(data_iter)
print(first_batch)

second_batch = next(data_iter)
print(second_batch) 

inputs, targets = next(data_iter)
print("Inputs:\n", inputs)
print("\nTargets:\n", targets)
'''

dataloader = create_dataloader_v1(
raw_text, batch_size=8, max_length=max_length,
stride=max_length, shuffle=False
)
data_iter = iter(dataloader)
inputs, targets = next(data_iter)
#print("Token IDs:\n", inputs)
#print("\nInputs shape:\n", inputs.shape)

token_embeddings = token_embedding_layer(inputs)
#print(token_embeddings.shape)

# 4. Positional Embeddings
# Transformers n√£o t√™m recorr√™ncia nem convolu√ß√µes, ent√£o precisamos indicar a posi√ß√£o dos tokens explicitamente.
context_length = max_length
pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)
pos_embeddings = pos_embedding_layer(torch.arange(context_length))
#print(pos_embeddings.shape)

# 5. Soma: token + posi√ß√£o
# Agora temos vetores com conte√∫do + posi√ß√£o, prontos para entrar no modelo.
input_embeddings = token_embeddings + pos_embeddings
print(input_embeddings.shape)


'''
1. Por que precisamos de "token embeddings" e "positional embeddings"?
- Token embeddings codificam o que est√° sendo dito (sem√¢ntica).
- Positional embeddings codificam a ordem das palavras (posi√ß√£o na sequ√™ncia).

Transformers processam tudo em paralelo, ent√£o sem posi√ß√£o, a ordem se perde ‚Äî "Eu amo voc√™" vira indistingu√≠vel de
 "Voc√™ amo eu".

2. Podemos calcular quantos par√¢metros esse modelo tem?
Sim! No estado atual (apenas embeddings), o n√∫mero de par√¢metros √©:

üì¶ Token embedding:
vocab_size = 50257
output_dim = 256
token_embedding_params = 50257 x 256 = 12,653,312

üì¶ Positional embedding:
context_length = 4
pos_embedding_params = 4 * 256 = 1,024

‚úÖ Total atual: 12,653,312 + 1,024 = 12,654,336 par√¢metros


'''