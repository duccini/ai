# Build a Large Language Model - Practice

## Chapter 2: Working with text data

üìå Resumo r√°pido sobre Byte Pair Encoding (BPE)
O algoritmo BPE funciona assim:

Come√ßa com a tokeniza√ß√£o caractere a caractere.

Em seguida, faz merges de pares de caracteres mais frequentes com base em um vocabul√°rio treinado.

Isso continua at√© que a string seja mapeada com os pares mais pr√≥ximos do vocabul√°rio aprendido.

Palavras desconhecidas n√£o s√£o ignoradas ‚Äî s√£o divididas em sub-palavras ou at√© caracteres individuais.

No caso do tiktoken, o modelo GPT-2 usa um vocabul√°rio BPE com cerca de 50 mil tokens.

<br>

---

üß™ Testando "Akwirw ier"

```python
import tiktoken

tokenizer = tiktoken.get_encoding("gpt2")
word = "Akwirw ier"
integers_word = tokenizer.encode(word)

for w in integers_word:
  print(f"{w}: {tokenizer.decode([w])}")
```

Sa√≠da t√≠pica (pode variar um pouco dependendo da vers√£o do vocabul√°rio):

```bash
10843: Ak
45960: wir
1101: w
220:  ier
```

Explica√ß√£o:

10843: Ak ‚Äì "Ak" √© um token conhecido no vocabul√°rio.

45960: wir ‚Äì "wir" √© outro sub-token presente.

1101: w ‚Äì o √∫ltimo w de "Akwirw" ficou isolado porque a sequ√™ncia "irw" n√£o existe no vocabul√°rio como um todo.

220: ier ‚Äì com o espa√ßo no in√≠cio, " ier" √© um token comum.

<br>

---

üîç Porque "Akwirw ier" √© quebrada assim?

1. GPT-2 n√£o conhece a palavra inteira "Akwirw".

2. BPE tenta quebr√°-la da melhor forma poss√≠vel com tokens conhecidos.

3. espa√ßo antes de "ier" √© importante: o modelo trata " ier" como um token diferente de "ier".
